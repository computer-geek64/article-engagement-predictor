{"name":"ML Project","tagline":"CS 4641 Project - Team 31","body":"# Introduction:\r\n\r\nThe New York Times is an American newspaper that is widely regarded as one of the popular and well-established news sources in the world. Over the past few years, user engagement has steadily grown leading to stronger online opinions as well as the need to classify and measure the sort of engagement on articles. By predicting the public response, the newspaper can give more targeted articles which will draw in more readers and keep the public well informed and engaged about critical current events in the world. To do so, we will use an open-source dataset of 16,000 New York Times articles and 5 million corresponding comments, with over 30 features to examine. \r\n\r\n# Problem Definition:\r\n\r\nGiven various features of a New York Times article as inputs, the goal of our project is to generate an overall **engagement metric**. This engagement metric can provide editors with necessary insight and feedback on their writing to gauge the audience reaction before they publicly publish their article.\r\n\r\n* Article Features: headline, article newsdesk, article section, article length, keywords, abstract, etc\r\n* Output of the first model: Number of comments for the article \r\n* Output of the second model: Forecasted average sentiment magnitude\r\n* Final output: Engagement metric (calculated through a weighted sum of the number of comments and the magnitude of the sentiment that the article is forecasted to receive)\r\n\r\n# Data\r\nTwo CSV files from Kaggle, articles.csv and comments.csv. These are linked through unique IDs which correspond to the info about articles and the individual comments corresponding to each comment. \r\n* Article columns - newsdesk, section, subsection, material, headline, abstract, keyword, word count, publish date, number comments, link ID\r\n* Comment columns - comment ID, status, user ID, flagged, trusted, comment body, link ID\r\n\r\n# Project Flow / Methods\r\n## Preprocessing\r\n\r\n:heavy_check_mark:**Data Cleaning:**\r\n* Removing incomplete features that don’t have data points for every column\r\n* Trim whitespace on raw text\r\n* Expand contractions and similar constructs \r\n* Lowercase all text, remove non-English characters \r\n* Convert English number words to actual numbers \r\n* Remove stopwords and words of length <= 2\r\n\r\n:heavy_check_mark:**Feature Engineering:**\r\nDrop appropriate columns from each CSV with justification\r\n* Article Columns Drop Justification\r\n  * Newsdesk = Section column is a greater encompassing feature for the article’s area\r\n  * Material = Drop every row of Material that has one of these: ['Interactive Feature', 'Obituary (Obit)', 'briefing', 'Letter'] as these are not news articles intended for engagement prediction\r\n  * Keywords = The abstract is better suited for an NLP model to extract useful information rather than uncontextualized keywords\r\n  * Publication Date = If we are looking to gain insight on an article about to be published, the date of past articles will not help us because we cannot use the date to understand why an article had a certain sentiment since we don’t know what events occurred around that date.\r\n* Comment Columns Drop Justification\r\n  * Drop everything except comment body and link ID. We only need to have the comment content as well as the corresponding article to generate a sentiment value based on the comment text. The link ID will not be used in actually building the model, but only for linking an article with its corresponding comments.\r\n  \r\nInitial tests of our project will use all the features listed above, since feature selection will be performed after the later parts of the project have been completed (building the models to predict the number of words and the sentiment). We plan to use the [Extra Trees Regressor's](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) ability to fit an estimator and provide feature importances in order to select which features may be more or less relevant in our final model.    \r\n  \r\n:heavy_check_mark:**Generation of Sentiment Column:**\r\nWe ran a sentiment analysis model that will generate the sentiment column in the articles dataset based on each comment body (in comments dataset). A [transformers sentiment model](https://huggingface.co/transformers/) was used for each comment row with the comment text as input features. A new column “average comment sentiment” was added in the article dataset, which averages all the sentiment columns for the comments of that article.  \r\n\r\n:heavy_check_mark:**Datasets after preprocessing and adding the sentiment column:**\r\nMore than 90% of the dataset was found to still be usable for the predictions of the number of words and the sentiment. All of the preprocessing steps are robust in that they help standardize the dataset by removing or modifying anomalies (converting all numbers to digit form, removing non-English characters, removing less relevant words). By dropping features that are not directly relevant to the problem and sanitizing the dataset, we can ensure our model can make more useful predictions. A short example of our preprocessing capability is the input sentence:\r\n\r\n> \"Thirty white horses, on a red hill. First they champ, then they stamp, then they stand still.\"\r\n\r\nAfter preprocessing, it becomes:\r\n\r\n> \"30 white horses red hill first champ stamp stand still\"\r\n\r\nwhich is more semantically useful for our model.\r\n\r\n\r\n\r\n## Prediction of Number of Words and Sentiment ##\r\n**Number of Comments Model:**\r\nAfter midterm report - use numerical features and text features together (columns which were not dropped) as features to predict # of comments for a given new article. \r\n\r\n**Sentiment Prediction Model:**\r\nAfter midterm report - use numerical features and text features (including number of comments and not dropped columns) as features together to predict “average sentiment” for a new article. \r\n\r\n**Engagement Metric Calculation:**\r\nFor a given new article, run number of comments model first and use the predicted comment value as one of the input features into the sentiment prediction model to get predicted sentiment value. The engagement metric will then be calculated based on number of comments and predicted sentiment value.\r\n\r\n\r\n# Discussion:\r\n\r\nBy analyzing and predicting the number of comments anticipated for an article, the publishers can determine the level of public interest in a given topic and can choose to follow up with corresponding related articles. Additionally, the sentiment analysis allows  the editors to determine how polarizing an article is, and potentially provide items such as blog posts to provide a forum for further public discussion. Low sentiment can tell the editor that perhaps the article should be revised to make it more engaging.\r\n\r\n# Dataset:\r\n\r\nThe dataset is too large (several gigabytes in size) to upload to the GitHub repository, in both its raw and preprocessed forms.\r\nFurthermore, it is bad practice to integrate files of immense sizes into version control systems, because the commit graph can expand at an exponential rate when merging and executing other complex Git actions.\r\n\r\nTherefore, the actual dataset can be downloaded from Kaggle ([https://www.kaggle.com/benjaminawd/new-york-times-articles-comments-2020](https://www.kaggle.com/benjaminawd/new-york-times-articles-comments-2020)), and we demonstrated the main part of our preprocessing model in the preprocessing section above.\r\n\r\n# References:\r\n\r\nTsagkias, M., Weerkamp, W., & De Rijke, M. (2009, November). Predicting the volume of comments on online news stories. In Proceedings of the 18th ACM conference on Information and knowledge management (pp. 1765-1768).\r\n\r\nSchumaker, R. P., Zhang, Y., Huang, C. N., & Chen, H. (2012). Evaluating sentiment in financial news articles. Decision Support Systems, 53(3), 458-464.\r\n\r\nAlthaus, S. L., & Tewksbury, D. (2002). Agenda Setting and the “New” News: Patterns of Issue Importance Among Readers of the Paper and Online Versions of the New York Times. Communication Research, 29(2), 180–207. https://doi.org/10.1177/0093650202029002004\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}