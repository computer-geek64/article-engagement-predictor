{"name":"ML Project","tagline":"CS 4641 Project - Team 31","body":"# Introduction:\r\n\r\nThe New York Times is an American newspaper that is widely regarded as one of the popular and well-established news sources in the world. Over the past few years, user engagement has steadily grown leading to stronger online opinions as well as the need to classify and measure the sort of engagement on articles. By predicting the public response, the newspaper can give more targeted articles which will draw in more readers and keep the public well informed and engaged about critical current events in the world. To do so, we will use an open-source dataset of 16,000 New York Times articles and 5 million corresponding comments, with over 30 features to examine. \r\n\r\n# Problem Definition:\r\n\r\nGiven various features of a New York Times article as inputs, the goal of our project is to generate an overall **engagement metric**. This engagement metric can provide editors with necessary insight and feedback on their writing to gauge the audience reaction before they publicly publish their article.\r\n\r\n* Article Features: headline, article newsdesk, article section, article length, keywords, abstract, etc\r\n* Output of the first model: Number of comments for the article \r\n* Output of the second model: Forecasted average sentiment magnitude\r\n* Final output: Engagement metric (calculated through a weighted sum of the number of comments and the magnitude of the sentiment that the article is forecasted to receive)\r\n\r\n# Data\r\nTwo CSV files from Kaggle, articles.csv and comments.csv. These are linked through unique IDs which correspond to the info about articles and the individual comments corresponding to each comment. \r\n* Article columns - newsdesk, section, subsection, material, headline, abstract, keyword, word count, publish date, number comments, link ID\r\n* Comment columns - comment ID, status, user ID, flagged, trusted, comment body, link ID\r\n\r\n![material](images/ny_times.PNG)\r\n\r\n# Project Flow / Methods\r\n## Preprocessing\r\n\r\n:heavy_check_mark:**Data Cleaning:**\r\n* Removing incomplete features that don’t have data points for every column\r\n* Trim whitespace on raw text\r\n* Expand contractions and similar constructs \r\n* Lowercase all text, remove non-English characters \r\n* Convert English number words to actual numbers \r\n* Remove stopwords and words of length <= 2\r\n\r\n:heavy_check_mark:**Feature Engineering:**\r\nDrop appropriate columns from each CSV with justification\r\n* Article Columns Drop Justification\r\n  * Newsdesk = Section column is a greater encompassing feature for the article’s area\r\n  * Material = Drop every row of Material that has one of these: ['Interactive Feature', 'Obituary (Obit)', 'briefing', 'Letter'] as these are not news articles intended for engagement prediction\r\n  * Keywords = The abstract is better suited for an NLP model to extract useful information rather than uncontextualized keywords\r\n  * Publication Date = If we are looking to gain insight on an article about to be published, the date of past articles will not help us because we cannot use the date to understand why an article had a certain sentiment since we don’t know what events occurred around that date.\r\n* Comment Columns Drop Justification\r\n  * Drop everything except comment body and link ID. We only need to have the comment content as well as the corresponding article to generate a sentiment value based on the comment text. The link ID will not be used in actually building the model, but only for linking an article with its corresponding comments. \r\nAdditionally, we want to convert our text-based features to numerical representations for the model to use. To do this, we represent the “material” feature using one-hot encoding. We first condense the material section into 3 main categories: News, Op-Ed, and Other (since the other remaining material sections have very few entries). When doing this, we add 3 new columns to the article data.\r\n\r\n  * For example, the frequencies of the \"material\" section shown in the first graph below indicates that the non News/Op-Ed articles are very low in frequency and thus were combined into a new material type. Similarly, the subsection column had more than half the articles with null values, and thus the column was dropped due to its insignificance.\r\n\r\n![material](images/mat_freq_v_art_no.png)\r\n![subsection](images/subsection.png)\r\n\r\n:heavy_check_mark:**Generation of Sentiment Features:**\r\nWe ran a sentiment analysis model that will generate the sentiment column in the articles dataset based on each comment body (in comments dataset). A [transformers sentiment model](https://huggingface.co/transformers/) was used for each comment row with the comment text as input features. This is a very simple model which  returns a single value per column row  gauging the sentiment feeling of each comment. A new column “average comment sentiment” was added in the article dataset, which for each article, represents the average of all sentiment values for the corresponding comments. This average comment sentiment column was scaled from -1 to 1 around an average of 0 for consistency across articles. \r\n\r\n:heavy_check_mark:**Converting Comments to a Numerical Representation using Word Embedding**\r\nWith this section, we want to add numerical features which represent the context of a given article. This is so that for later models, we don’t have to train on both text and numerical features, but can instead train on the existing numerical features such as word count as well as the added numerical embedded features. To do this, we use a transformer model which takes in the two text features ‘headline’ and ‘abstract’ and outputs a vector of 768 features representing this text. This was accomplished using a [sentence transformer framework](https://www.sbert.net/docs/quickstart.html), Sentence-BERT, that uses the DistilBERT-base-uncased model that comes pretrained on a large dataset of sentences. Average pooling was performed to provide a vector of 768 word embeddings for a sentence of any size. Thus, each article in the article dataset will now have 768 more numerical features or columns.\r\n\r\n\r\n:heavy_check_mark:**Datasets after preprocessing and adding the sentiment features:**\r\nAll of the preprocessing steps are robust in that they help standardize the dataset by removing or modifying anomalies (converting all numbers to digit form, removing non-English characters, removing less relevant words). By dropping features that are not directly relevant to the problem and sanitizing the dataset, we can ensure our model can make more useful predictions. A short example of our preprocessing capability is the input sentence:\r\n\r\n> \"Thirty white horses, on a red hill. First they champ, then they stamp, then they stand still.\"\r\n\r\nAfter preprocessing, it becomes:\r\n\r\n> \"30 white horses red hill first champ stamp stand still\"\r\n\r\nwhich is more semantically useful for our model. When all these preprocessing measures were applied on the full dataset, articles.csv and comments.csv were reduced to\r\n\r\nArticles_cleaned.csv -> Insert\r\nComments_cleaned.csv -> insert\r\n\r\n\r\n\r\n\r\n\r\n## Prediction of Number of Words and Sentiment ##\r\n**Number of Comments Model:**\r\n The goal of this model is to train on existing articles with every single numerical feature in order to predict the number of comments. These features include word count, average sentiment, the 768 features from word embedding, and the one hot encoding for material. The type of model used was an extreme gradient boost decision tree ([XGBoost](https://xgboost.readthedocs.io/en/latest/)) with 70% train data, 15% validation, and 15% test. We use Bayesian hyperparameter optimization for number of estimators, max depth, and learning rate. We can use this tuned model to predict the number of comments for a given article with all these input features. \r\n\r\n**Sentiment Prediction Model:**\r\nThe goal and structure of the sentiment prediction model is very similar to the number of comments model. Once again, we train on the existing articles with all the numerical features but this time to predict the sentiment response. We once again use an XGBoost model with similar data divisions along with hyperparameter optimization. However, the difference is that there is an additional input feature column number of comments which we are also using to train sentiment on.  \r\n\r\n**Engagement Metric Calculation:**\r\nNote that at this stage, we are equipped to be given a new article with a headline and abstract and run these series of models to predict user response. We can use the number of comments model to in turn train the sentiment prediction model, which will give us a sentiment value which is an estimate of how the target audience will view the article. A final engagement metric column will then be calculated based on the number of comments and predicted sentiment value which is the final indicator of the success of an article meant to be tested on. \r\n\r\n# Results:\r\nAfter the entire process is complete, we end up with a newly calculated Engagement Metric for each article. This number represents how much traction each article generates. When we want to take a new unpublished article and determine what its engagement might be, we use our 2 models and newly generated columns to predict the number of comments and the sentiment of the article.\r\n\r\n\r\n# Discussion:\r\n\r\nBy analyzing and predicting the number of comments anticipated for an article, the publishers can determine the level of public interest in a given topic and can choose to follow up with corresponding related articles. Additionally, the sentiment analysis allows  the editors to determine how polarizing an article is, and potentially provide items such as blog posts to provide a forum for further public discussion. Low sentiment can tell the editor that perhaps the article should be revised to make it more engaging.\r\n\r\n# Dataset:\r\n\r\nThe dataset is too large (several gigabytes in size) to upload to the GitHub repository, in both its raw and preprocessed forms.\r\nFurthermore, it is bad practice to integrate files of immense sizes into version control systems, because the commit graph can expand at an exponential rate when merging and executing other complex Git actions.\r\n\r\nTherefore, the actual dataset can be downloaded from Kaggle ([https://www.kaggle.com/benjaminawd/new-york-times-articles-comments-2020](https://www.kaggle.com/benjaminawd/new-york-times-articles-comments-2020)), and we demonstrated the main part of our preprocessing model in the preprocessing section above.\r\n\r\n# References:\r\n\r\nTsagkias, M., Weerkamp, W., & De Rijke, M. (2009, November). Predicting the volume of comments on online news stories. In Proceedings of the 18th ACM conference on Information and knowledge management (pp. 1765-1768).\r\n\r\nSchumaker, R. P., Zhang, Y., Huang, C. N., & Chen, H. (2012). Evaluating sentiment in financial news articles. Decision Support Systems, 53(3), 458-464.\r\n\r\nAlthaus, S. L., & Tewksbury, D. (2002). Agenda Setting and the “New” News: Patterns of Issue Importance Among Readers of the Paper and Online Versions of the New York Times. Communication Research, 29(2), 180–207. https://doi.org/10.1177/0093650202029002004\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}